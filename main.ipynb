{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import wikipediaapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "#NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization and preprocessing\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def apply_stemming(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def construct_bag_of_words(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_representation = vectorizer.fit_transform([text])\n",
    "    return bow_representation.toarray()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slicing input\n",
    "def divide_into_slices(input_text, standard_size):\n",
    "    tokens = tokenize_text(input_text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = apply_stemming(tokens)\n",
    "    bag_of_words = construct_bag_of_words(tokens)\n",
    "    \n",
    "    if len(bag_of_words) <= standard_size:\n",
    "        return [bag_of_words]\n",
    "    \n",
    "    # otherwise, divide the processed input\n",
    "    num_slices = len(bag_of_words) // standard_size + 1\n",
    "    slice_size = len(bag_of_words) // num_slices\n",
    "    slices = [bag_of_words[i:i+slice_size] for i in range(0, len(bag_of_words), slice_size)]\n",
    "    \n",
    "    return slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Distance Calculation\n",
    "def calculate_cosine_distance(slice1, slice2):\n",
    "    similarity_matrix = cosine_similarity(np.array(slice1).reshape(1, -1), np.array(slice2).reshape(1, -1))\n",
    "    cosine_distance = 1 - similarity_matrix[0, 0]\n",
    "    return cosine_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Slicing Criteria\n",
    "def check_slicing_criteria(slices, cosine_distance_threshold=0.2):\n",
    "    new_slices = [slices[0]] \n",
    "\n",
    "    for i in range(1, len(slices)):\n",
    "        current_slice = slices[i]\n",
    "        previous_slice = new_slices[-1]\n",
    "\n",
    "        # checking slices overlaping\n",
    "        if current_slice[0] >= previous_slice[-1]:\n",
    "            new_slices.append(current_slice)\n",
    "        else:\n",
    "            distance = calculate_cosine_distance(previous_slice, current_slice)\n",
    "\n",
    "            if distance > cosine_distance_threshold:\n",
    "                new_slices[-1] = current_slice\n",
    "\n",
    "    return new_slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP Pipeline\n",
    "def nlp_pipeline(input_text, standard_size=128, cosine_distance_threshold=0.2):\n",
    "    tokens = tokenize_text(input_text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = apply_stemming(tokens)\n",
    "    bag_of_words = construct_bag_of_words(tokens)\n",
    "    \n",
    "    if len(bag_of_words) <= standard_size:\n",
    "        return [bag_of_words]\n",
    "    \n",
    "    slices = divide_into_slices(input_text, standard_size)\n",
    "    slices = check_slicing_criteria(slices, cosine_distance_threshold)\n",
    "    \n",
    "    return slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "def get_wikipedia_documents(topic, num_paragraphs=3):\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI, headers={'User-Agent': 'emilalizada0@gmail.com'})\n",
    "\n",
    "    page_py = wiki_wiki.page(topic)\n",
    "\n",
    "    if not page_py.exists():\n",
    "        return None\n",
    "\n",
    "    paragraphs = []\n",
    "    for section in page_py.sections:\n",
    "        paragraphs.extend(section.text.split('\\n')[:num_paragraphs])\n",
    "\n",
    "    return ' '.join(paragraphs)\n",
    "\n",
    "\n",
    "geographical_topics = [\"Geography\", \"Sea\", \"Ocean\", \"Longitude\", \"Meteorology\", \"Climate\"]\n",
    "non_geographical_topics = [\"Artificial intelligence\", \"Computer Science\", \"Medical\", \"History\"]\n",
    "geographical_documents = []\n",
    "non_geographical_documents = []\n",
    "\n",
    "# Fetch documents for geographical topics\n",
    "for topic in geographical_topics:\n",
    "    document = get_wikipedia_documents(topic)\n",
    "    if document:\n",
    "        geographical_documents.append(document)\n",
    "    else:\n",
    "        print(f\"Could not retrieve document for {topic}\")\n",
    "\n",
    "# Fetch documents for non-geographical topics\n",
    "for topic in non_geographical_topics:\n",
    "    document = get_wikipedia_documents(topic)\n",
    "    if document:\n",
    "        non_geographical_documents.append(document)\n",
    "    else:\n",
    "        print(f\"Could not retrieve document for {topic}\")\n",
    "\n",
    "# Display the obtained documents\n",
    "print(\"Geographical Documents:\")\n",
    "for i, document in enumerate(geographical_documents, start=1):\n",
    "    print(f\"Document {i} ({geographical_topics[i-1]}):\")\n",
    "    print(document)\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "print(\"\\nNon-Geographical Documents:\")\n",
    "for i, document in enumerate(non_geographical_documents, start=1):\n",
    "    print(f\"Document {i} ({non_geographical_topics[i-1]}):\")\n",
    "    print(document)\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "# Select one document from each category for NLP pipeline\n",
    "sample_input1 = geographical_documents[0] if geographical_documents else None\n",
    "sample_input2 = non_geographical_documents[0] if non_geographical_documents else None\n",
    "\n",
    "if sample_input1:\n",
    "    # Using the NLP pipeline for the first geographical document\n",
    "    result_slices1 = nlp_pipeline(sample_input1)\n",
    "    print(\"Input Text (Geographical):\")\n",
    "    print(sample_input1)\n",
    "    print(\"\\nResulting Slices:\")\n",
    "    for i, slice in enumerate(result_slices1, start=1):\n",
    "        print(f\"Slice {i}: {slice}\")\n",
    "\n",
    "if sample_input2:\n",
    "    result_slices2 = nlp_pipeline(sample_input2)\n",
    "    print(\"Input Text (Non-Geographical):\")\n",
    "    print(sample_input2)\n",
    "    print(\"\\nResulting Slices:\")\n",
    "    for i, slice in enumerate(result_slices2, start=1):\n",
    "        print(f\"Slice {i}: {slice}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with Different Inputs\n",
    "cosine_distance_threshold = 0.2\n",
    "\n",
    "input_below_standard_size = sample_input1\n",
    "slices_below_standard_size = nlp_pipeline(input_below_standard_size)\n",
    "\n",
    "input_above_standard_size = sample_input1 * 100\n",
    "slices_above_standard_size = nlp_pipeline(input_above_standard_size)\n",
    "\n",
    "#verify bag-of-words representations for slices\n",
    "print(\"Bag of Words for Slices Below Standard Size:\")\n",
    "for i, slice in enumerate(slices_below_standard_size):\n",
    "    print(f\"Slice {i}: {slice}\")\n",
    "\n",
    "print(\"\\nBag of Words for Slices Above Standard Size:\")\n",
    "for i, slice in enumerate(slices_above_standard_size):\n",
    "    print(f\"Slice {i}: {slice}\")\n",
    "\n",
    "print(\"\\nTesting completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
