{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\emila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emila\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing libraries\n",
    "import wikipediaapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "#NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization and preprocessing\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def apply_stemming(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def construct_bag_of_words(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_representation = vectorizer.fit_transform([text])\n",
    "    return bow_representation.toarray()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slicing input\n",
    "def divide_into_slices(input_text, standard_size):\n",
    "    tokens = tokenize_text(input_text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = apply_stemming(tokens)\n",
    "    bag_of_words = construct_bag_of_words(tokens)\n",
    "    \n",
    "    if len(bag_of_words) <= standard_size:\n",
    "        return [bag_of_words]\n",
    "    \n",
    "    # otherwise, divide the processed input\n",
    "    num_slices = len(bag_of_words) // standard_size + 1\n",
    "    slice_size = len(bag_of_words) // num_slices\n",
    "    slices = [bag_of_words[i:i+slice_size] for i in range(0, len(bag_of_words), slice_size)]\n",
    "    \n",
    "    return slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Distance Calculation\n",
    "def calculate_cosine_distance(slice1, slice2):\n",
    "    similarity_matrix = cosine_similarity(np.array(slice1).reshape(1, -1), np.array(slice2).reshape(1, -1))\n",
    "    cosine_distance = 1 - similarity_matrix[0, 0]\n",
    "    return cosine_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking Slicing Criteria\n",
    "def check_slicing_criteria(slices, cosine_distance_threshold=0.2):\n",
    "    new_slices = [slices[0]] \n",
    "\n",
    "    for i in range(1, len(slices)):\n",
    "        current_slice = slices[i]\n",
    "        previous_slice = new_slices[-1]\n",
    "\n",
    "        # checking slices overlaping\n",
    "        if current_slice[0] >= previous_slice[-1]:\n",
    "            new_slices.append(current_slice)\n",
    "        else:\n",
    "            distance = calculate_cosine_distance(previous_slice, current_slice)\n",
    "\n",
    "            if distance > cosine_distance_threshold:\n",
    "                new_slices[-1] = current_slice\n",
    "\n",
    "    return new_slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP Pipeline\n",
    "def nlp_pipeline(input_text, standard_size=128, cosine_distance_threshold=0.2):\n",
    "    tokens = tokenize_text(input_text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = apply_stemming(tokens)\n",
    "    bag_of_words = construct_bag_of_words(tokens)\n",
    "    \n",
    "    if len(bag_of_words) <= standard_size:\n",
    "        return [bag_of_words]\n",
    "    \n",
    "    slices = divide_into_slices(input_text, standard_size)\n",
    "    slices = check_slicing_criteria(slices, cosine_distance_threshold)\n",
    "    \n",
    "    return slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chemistry Documents:\n",
      "Document 1 (Chemistry):\n",
      "The word chemistry comes from a modification during the Renaissance of the word alchemy, which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism, and medicine. Alchemy is often associated with the quest to turn lead or other base metals into gold, though alchemists were also interested in many of the questions of modern chemistry.The modern word alchemy in turn is derived from the Arabic word al-kīmīā (الكیمیاء). This may have Egyptian origins since al-kīmīā is derived from the Ancient Greek χημία, which is in turn derived from the word Kemet, which is the ancient name of Egypt in the Egyptian language. Alternately, al-kīmīā may derive from χημεία 'cast together'. The current model of atomic structure is the quantum mechanical model. Traditional chemistry starts with the study of elementary particles, atoms, molecules, substances, metals, crystals and other aggregates of matter. Matter can be studied in solid, liquid, gas and plasma states, in isolation or in combination. The interactions, reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory. The chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it.  A chemical reaction is a transformation of some substances into one or more different substances. The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws. Energy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists. Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are: The history of chemistry spans a period from very old times to the present. Since several millennia BC, civilizations were using technologies that would eventually form the basis of the various branches of chemistry. Examples include extracting metals from ores, making pottery and glazes, fermenting beer and wine, extracting chemicals from plants for medicine and perfume, rendering fat into soap, making glass, and making alloys like bronze. Chemistry was preceded by its protoscience, alchemy, which operated a non-scientific approach to understanding the constituents of matter and their interactions. Despite being unsuccessful in explaining the nature of matter and its transformations, alchemists set the stage for modern chemistry by performing experiments and recording the results. Robert Boyle, although skeptical of elements and convinced of alchemy, played a key part in elevating the \"sacred art\" as an independent, fundamental and philosophical discipline in his work The Sceptical Chymist (1661).While both alchemy and chemistry are concerned with matter and its transformations, the crucial difference was given by the scientific method that chemists employed in their work. Chemistry, as a body of knowledge distinct from alchemy, became an established science with the work of Antoine Lavoisier, who developed a law of conservation of mass that demanded careful measurement and quantitative observations of chemical phenomena. The history of chemistry afterwards is intertwined with the history of thermodynamics, especially through the work of Willard Gibbs.     Popular reading  Atkins, P.W. Galileo's Finger (Oxford University Press) ISBN 0-19-860941-8 Atkins, P.W. Atkins' Molecules (Cambridge University Press) ISBN 0-521-82397-8 Kean, Sam. The Disappearing Spoon – and Other True Tales from the Periodic Table (Black Swan) London, 2010 ISBN 978-0-552-77750-6 Levi, Primo The Periodic Table (Penguin Books) [1975] translated from the Italian by Raymond Rosenthal (1984) ISBN 978-0-14-139944-7  General Chemistry principles, patterns and applications.\n",
      "\n",
      "---\n",
      "\n",
      "Document 2 (Acids):\n",
      "Modern definitions are concerned with the fundamental chemical reactions common to all acids. Most acids encountered in everyday life are aqueous solutions, or can be dissolved in water, so the Arrhenius and Brønsted–Lowry definitions are the most relevant. The Brønsted–Lowry definition is the most widely used definition; unless otherwise specified, acid–base reactions are assumed to involve the transfer of a proton (H+) from an acid to a base. Hydronium ions are acids according to all three definitions. Although alcohols and amines can be Brønsted–Lowry acids, they can also function as Lewis bases due to the lone pairs of electrons on their oxygen and nitrogen atoms. Reactions of acids are often generalized in the form HA  ⇌  H+ + A−, where HA represents the acid and A− is the conjugate base. This reaction is referred to as protolysis. The protonated form (HA) of an acid is also sometimes referred to as the free acid.Acid–base conjugate pairs differ by one proton, and can be interconverted by the addition or removal of a proton (protonation and deprotonation, respectively). Note that the acid can be the charged species and the conjugate base can be neutral in which case the generalized reaction scheme could be written as HA+  ⇌  H+ + A. In solution there exists an equilibrium between the acid and its conjugate base. The equilibrium constant K is an expression of the equilibrium concentrations of the molecules or the ions in solution. Brackets indicate concentration, such that [H2O] means the concentration of H2O. The acid dissociation constant Ka is generally used in the context of acid–base reactions. The numerical value of Ka is equal to the product (multiplication) of the concentrations of the products divided by the concentration of the reactants, where the reactant is the acid (HA) and the products are the conjugate base and H+.                          Arrhenius acids are named according to their anions. In the classical naming system, the ionic suffix is dropped and replaced with a new suffix, according to the table following. The prefix \"hydro-\" is used when the acid is made up of just hydrogen and one other element. For example, HCl has chloride as its anion, so the hydro- prefix is used, and the -ide suffix makes the name take the form hydrochloric acid. Classical naming system:  In the IUPAC naming system, \"aqueous\" is simply added to the name of the ionic compound. Thus, for hydrogen chloride, as an acid solution, the IUPAC name is aqueous hydrogen chloride. The strength of an acid refers to its ability or tendency to lose a proton. A strong acid is one that completely dissociates in water; in other words, one mole of a strong acid HA dissolves in water yielding one mole of H+ and one mole of the conjugate base, A−, and none of the protonated acid HA. In contrast, a weak acid only partially dissociates and at equilibrium both the acid and the conjugate base are in solution. Examples of strong acids are hydrochloric acid (HCl), hydroiodic acid (HI), hydrobromic acid (HBr), perchloric acid (HClO4), nitric acid (HNO3) and sulfuric acid (H2SO4). In water each of these essentially ionizes 100%. The stronger an acid is, the more easily it loses a proton, H+. Two key factors that contribute to the ease of deprotonation are the polarity of the H—A bond and the size of atom A, which determines the strength of the H—A bond. Acid strengths are also often discussed in terms of the stability of the conjugate base. Stronger acids have a larger acid dissociation constant, Ka and a lower pKa than weaker acids. Sulfonic acids, which are organic oxyacids, are a class of strong acids. A common example is toluenesulfonic acid (tosylic acid). Unlike sulfuric acid itself, sulfonic acids can be solids. In fact, polystyrene functionalized into polystyrene sulfonate is a solid strongly acidic plastic that is filterable. Superacids are acids stronger than 100% sulfuric acid. Examples of superacids are fluoroantimonic acid, magic acid and perchloric acid. Superacids can permanently protonate water to give ionic, crystalline hydronium \"salts\". They can also quantitatively stabilize carbocations. While Ka measures the strength of an acid compound, the strength of an aqueous acid solution is measured by pH, which is an indication of the concentration of hydronium in the solution.  The pH of a simple solution of an acid compound in water is determined by the dilution of the compound and the compound's Ka. Lewis acids have been classified in the ECW model and it has been shown that there is no one order of acid strengths.  The relative acceptor strength of Lewis acids toward a series of bases, versus other Lewis acids, can be illustrated by C-B plots.  It has been shown that to define the order of Lewis acid strength at least two properties must be considered. For Pearson's qualitative HSAB theory the two properties are hardness and strength while for Drago's quantitative ECW model the two properties are electrostatic and covalent.  To determine the concentration of an acid in an aqueous solution, an acid–base titration is commonly performed. A strong base solution with a known concentration, usually NaOH or KOH, is added to neutralize the acid solution according to the color change of the indicator with the amount of base added. The titration curve of an acid titrated by a base has two axes, with the base volume on the x-axis and the solution's pH value on the y-axis. The pH of the solution always goes up as the base is added to the solution.  Many biologically important molecules are acids. Nucleic acids, which contain acidic phosphate groups, include DNA and RNA. Nucleic acids contain the genetic code that determines many of an organism's characteristics, and is passed from parents to offspring. DNA contains the chemical blueprint for the synthesis of proteins, which are made up of amino acid subunits. Cell membranes contain fatty acid esters such as phospholipids. An α-amino acid has a central carbon (the α or alpha carbon) that is covalently bonded to a carboxyl group (thus they are carboxylic acids), an amino group, a hydrogen atom and a variable group. The variable group, also called the R group or side chain, determines the identity and many of the properties of a specific amino acid. In glycine, the simplest amino acid, the R group is a hydrogen atom, but in all other amino acids it is contains one or more carbon atoms bonded to hydrogens, and may contain other elements such as sulfur, oxygen or nitrogen. With the exception of glycine, naturally occurring amino acids are chiral and almost invariably occur in the L-configuration. Peptidoglycan, found in some bacterial cell walls contains some D-amino acids. At physiological pH, typically around 7, free amino acids exist in a charged form, where the acidic carboxyl group (-COOH) loses a proton (-COO−) and the basic amine group (-NH2) gains a proton (-NH+3). The entire molecule has a net neutral charge and is a zwitterion, with the exception of amino acids with basic or acidic side chains. Aspartic acid, for example, possesses one protonated amine and two deprotonated carboxyl groups, for a net charge of −1 at physiological pH. Fatty acids and fatty acid derivatives are another group of carboxylic acids that play a significant role in biology. These contain long hydrocarbon chains and a carboxylic acid group on one end. The cell membrane of nearly all organisms is primarily made up of a phospholipid bilayer, a micelle of hydrophobic fatty acid esters with polar, hydrophilic phosphate \"head\" groups. Membranes contain additional components, some of which can participate in acid–base reactions. In humans and many other animals, hydrochloric acid is a part of the gastric acid secreted within the stomach to help hydrolyze proteins and polysaccharides, as well as converting the inactive pro-enzyme, pepsinogen into the enzyme, pepsin. Some organisms produce acids for defense; for example, ants produce formic acid. Acid–base equilibrium plays a critical role in regulating mammalian breathing. Oxygen gas (O2) drives cellular respiration, the process by which animals release the chemical potential energy stored in food, producing carbon dioxide (CO2) as a byproduct. Oxygen and carbon dioxide are exchanged in the lungs, and the body responds to changing energy demands by adjusting the rate of ventilation. For example, during periods of exertion the body rapidly breaks down stored carbohydrates and fat, releasing CO2 into the blood stream. In aqueous solutions such as blood CO2 exists in equilibrium with carbonic acid and bicarbonate ion.   Listing of strengths of common acids and bases Zumdahl, Steven S. (1997). Chemistry (4th ed.). Boston: Houghton Mifflin. ISBN 9780669417944. Pavia, D. L.; Lampman, G. M.; Kriz, G. S. (2004). Organic Chemistry Volume I. Mason, OH: Cengage Learning. ISBN 0759347271. Curtipot: Acid–Base equilibria diagrams, pH calculation and titration curves simulation and analysis – freeware\n",
      "\n",
      "---\n",
      "\n",
      "Document 3 (Kinetics):\n",
      "Kinetics (physics), the study of motion and its causes Rigid body kinetics, the study of the motion of rigid bodies Chemical kinetics, the study of chemical reaction rates Enzyme kinetics, the study of biochemical reaction rates catalysed by an enzyme Michaelis–Menten kinetics, the widely accepted general model of enzyme kinetics Goldbeter–Koshland kinetics, describe a steady-state solution for a 2-state biological system Kinetics (company), a technology company KinetX, an aerospace engineering company ST Kinetics, a weaponry and specialty vehicle manufacturer Color Kinetics, a former lighting company, now part of the Philips group of companies Kinetics (rapper), rapper and songwriter from New York City Kinetics Internet Protocol (KIP), an AppleTalk network protocol NASCAR Kinetics, a semester-long experiential program offered to several American universities Kinetic activity (military terminology) Dynamics (disambiguation) Kinetic (disambiguation) Kinematics, a branch of classical mechanics that describes the motion of particles (alternatively \"points\"), objects (\"bodies\"), and groups of objects (\"systems of bodies\") without considering the mass of each or the forces that caused the motion Analytical mechanics, a collection of closely related alternative formulations of classical mechanics Analytical dynamics, concerned about the relationship between motion of bodies and its causes, namely the forces acting on the bodies and the properties of the bodies (particularly mass and moment of inertia)\n",
      "\n",
      "---\n",
      "\n",
      "Document 4 (Atomic):\n",
      "Atomic (band), a Norwegian jazz quintet Atomic (Lit album), 2001 Atomic (Mogwai album), 2016 Atomic, an album by Rockets, 1982 Atomic (EP), by Labrinth, 2013 \"Atomic\" (song), by Blondie, 1979  All pages with titles beginning with Atomic All pages with titles containing Atomic Atom (disambiguation) Atomicity (database systems) Nuclear (disambiguation)\n",
      "\n",
      "---\n",
      "\n",
      "Document 5 (Stoichiometry):\n",
      "The term stoichiometry was first used by Jeremias Benjamin Richter in 1792 when the first volume of Richter's Anfangsgründe der Stöchyometrie, oder Messkunst chymischer Elemente (Fundamentals of Stoichiometry, or the Art of Measuring the Chemical Elements was published. The term is derived from the Ancient Greek words στοιχεῖον stoicheion \"element\" and μέτρον metron \"measure\". A stoichiometric amount or stoichiometric ratio of a reagent is the optimum amount or ratio where, assuming that the reaction proceeds to completion:  All of the reagent is consumed There is no deficiency of the reagent There is no excess of the reagent.Stoichiometry rests upon the very basic laws that help to understand it better, i.e., law of conservation of mass, the law of definite proportions (i.e., the law of constant composition), the law of multiple proportions and the law of reciprocal proportions. In general, chemical reactions combine in definite ratios of chemicals. Since chemical reactions can neither create nor destroy matter, nor transmute one element into another, the amount of each element must be the same throughout the overall reaction. For example, the number of atoms of a given element X on the reactant side must equal the number of atoms of that element on the product side, whether or not all of those atoms are actually involved in a reaction. Chemical reactions, as macroscopic unit operations, consist of simply a very large number of elementary reactions, where a single molecule reacts with another molecule. As the reacting molecules (or moieties) consist of a definite set of atoms in an integer ratio, the ratio between reactants in a complete reaction is also in integer ratio. A reaction may consume more than one molecule, and the stoichiometric number counts this number, defined as positive for products (added) and negative for reactants (removed). The unsigned coefficients are generally referred to as the stoichiometric coefficients.Each element has an atomic mass, and considering molecules as collections of atoms, compounds have a definite molar mass. By definition, the atomic mass of carbon-12 is 12 Da, giving a molar mass of 12 g/mol. The number of molecules per mole in a substance is given by the Avogadro constant, defined as 6.02214076×1023 mol−1. Thus, to calculate the stoichiometry by mass, the number of molecules required for each reactant is expressed in moles and multiplied by the molar mass of each to give the mass of each reactant per mole of reaction. The mass ratios can be calculated by dividing each by the total in the whole reaction. Stoichiometry is not only used to balance chemical equations but also used in conversions, i.e., converting from grams to moles using molar mass as the conversion factor, or from grams to milliliters using density.  For example, to find the amount of NaCl (sodium chloride) in 2.00 g, one would do the following:                          Stoichiometry is often used to balance chemical equations (reaction stoichiometry). For example, the two diatomic gases, hydrogen and oxygen, can combine to form a liquid, water, in an exothermic reaction, as described by the following equation:  2 H2 + O2  →  2 H2OReaction stoichiometry describes the 2:1:2 ratio of hydrogen, oxygen, and water molecules in the above equation. The molar ratio allows for conversion between moles of one substance and moles of another. For example, in the reaction  2 CH3OH + 3 O2  →  2 CO2 + 4 H2Othe amount of water that will be produced by the combustion of 0.27 moles of CH3OH is obtained using the molar ratio between CH3OH and H2O of 2 to 4. Stoichiometry can also be used to find the quantity of a product yielded by a reaction. If a piece of solid copper (Cu) were added to an aqueous solution of silver nitrate (AgNO3), the silver (Ag) would be replaced in a single displacement reaction forming aqueous copper(II) nitrate (Cu(NO3)2) and solid silver.  How much silver is produced if 16.00 grams of Cu is added to the solution of excess silver nitrate? The following steps would be used:  Write and balance the equation Mass to moles: Convert grams of Cu to moles of Cu Mole ratio: Convert moles of Cu to moles of Ag produced Stoichiometry is also used to find the right amount of one reactant to \"completely\" react with the other reactant in a chemical reaction – that is, the stoichiometric amounts that would result in no leftover reactants when the reaction takes place. An example is shown below using the thermite reaction,  Fe2O3 + 2 Al  →  Al2O3 + 2 FeThis equation shows that 1 mole of iron(III) oxide and 2 moles of aluminum will produce 1 mole of aluminium oxide and 2 moles of iron.  So, to completely react with 85.0 g of iron(III) oxide (0.532 mol), 28.7 g (1.06 mol) of aluminium are needed.          The limiting reagent is the reagent that limits the amount of product that can be formed and is completely consumed when the reaction is complete. An excess reactant is a reactant that is left over once the reaction has stopped due to the limiting reactant being exhausted. Consider the equation of roasting lead(II) sulfide (PbS) in oxygen (O2) to produce lead(II) oxide (PbO) and sulfur dioxide (SO2):  2 PbS + 3 O2  →  2 PbO + 2 SO2To determine the theoretical yield of lead(II) oxide if 200.0 g of lead(II) sulfide and 200.0 g of oxygen are heated in an open container:     Often, more than one reaction is possible given the same starting materials.  The reactions may differ in their stoichiometry.  For example, the methylation of benzene (C6H6), through a Friedel–Crafts reaction using AlCl3 as a catalyst, may produce singly methylated (C6H5CH3), doubly methylated (C6H4(CH3)2), or still more highly methylated (C6H6−n(CH3)n) products, as shown in the following example,  C6H6 + CH3Cl → C6H5CH3 + HCl C6H6 + 2 CH3Cl → C6H4(CH3)2 + 2 HCl C6H6 + n CH3Cl → C6H6−n(CH3)n + n HClIn this example, which reaction takes place is controlled in part by the relative concentrations of the reactants. In lay terms, the stoichiometric coefficient of any given component is the number of molecules and/or formula units that participate in the reaction as written. A related concept is the stoichiometric number (using IUPAC nomenclature), wherein the stoichiometric coefficient is multiplied by +1 for all products and by −1 for all reactants. For example, in the reaction CH4 + 2 O2 → CO2 + 2 H2O, the stoichiometric number of CH4 is −1, the stoichiometric number of O2 is −2, for CO2 it would be +1 and for H2O it is +2. In more technically precise terms, the stoichiometric number in a chemical reaction system of the i-th component is defined as          In complex reactions, stoichiometries are often represented in a more compact form called the stoichiometry matrix. The stoichiometry matrix is denoted by the symbol N.If a reaction network has n reactions and m participating molecular species, then the stoichiometry matrix will have correspondingly m rows and n columns. For example, consider the system of reactions shown below:  S1 → S2 5 S3 + S2 → 4 S3 + 2 S2 S3 → S4 Gas stoichiometry is the quantitative relationship (ratio) between reactants and products in a chemical reaction with reactions that produce gases. Gas stoichiometry applies when the gases produced are assumed to be ideal, and the temperature, pressure, and volume of the gases are all known. The ideal gas law is used for these calculations. Often, but not always, the standard temperature and pressure (STP) are taken as 0 °C and 1 bar and used as the conditions for gas stoichiometric calculations. Gas stoichiometry calculations solve for the unknown volume or mass of a gaseous product or reactant. For example, if we wanted to calculate the volume of gaseous NO2 produced from the combustion of 100 g of NH3, by the reaction:  4 NH3 (g) + 7 O2 (g)  →  4 NO2 (g) + 6 H2O (l)we would carry out the following calculations:     In the combustion reaction, oxygen reacts with the fuel, and the point where exactly all oxygen is consumed and all fuel burned is defined as the stoichiometric point. With more oxygen (overstoichiometric combustion), some of it stays unreacted. Likewise, if the combustion is incomplete due to lack of sufficient oxygen, fuel remains unreacted. (Unreacted fuel may also remain because of slow combustion or insufficient mixing of fuel and oxygen – this is not due to stoichiometry). Different hydrocarbon fuels have different contents of carbon, hydrogen and other elements, thus their stoichiometry varies. Oxygen makes up only 20.95% of the volume of air, and only 23.20% of its mass. The air-fuel ratios listed below are much higher than the equivalent oxygen-fuel ratios, due to the high proportion of inert gasses in the air.  Gasoline engines can run at stoichiometric air-to-fuel ratio, because gasoline is quite volatile and is mixed (sprayed or carburetted) with the air prior to ignition. Diesel engines, in contrast, run lean, with more air available than simple stoichiometry would require. Diesel fuel is less volatile and is effectively burned as it is injected. Non-stoichiometric compound Biochemical systems equation Chemical reaction Chemical equation Molecule Molar mass Zumdahl, Steven S. Chemical Principles. Houghton Mifflin, New York, 2005, pp 148–150. Internal Combustion Engine Fundamentals, John B. Heywood  Engine Combustion primer from the University of Plymouth Free Stoichiometry Tutorials from Carnegie Mellon's ChemCollective Stoichiometry Add-In for Microsoft Excel Archived 2011-05-11 at the Wayback Machine for calculation of molecular weights, reaction coëfficients and stoichiometry. Reaction Stoichiometry Calculator a comprehensive free online reaction stoichiometry calculator. Stoichiometry Plus a stoichiometry calculator and more for Android.\n",
      "\n",
      "---\n",
      "\n",
      "Document 6 (Electrochemistry):\n",
      "  An electrochemical cell is a device that produces an electric current from energy released by a spontaneous redox reaction. This kind of cell includes the Galvanic cell or Voltaic cell, named after Luigi Galvani and Alessandro Volta, both scientists who conducted experiments on chemical reactions and electric current during the late 18th century. Electrochemical cells have two conductive electrodes (the anode and the cathode). The anode is defined as the electrode where oxidation occurs and the cathode is the electrode where the reduction takes place. Electrodes can be made from any sufficiently conductive materials, such as metals, semiconductors, graphite, and even conductive polymers. In between these electrodes is the electrolyte, which contains ions that can freely move. The galvanic cell uses two different metal electrodes, each in an electrolyte where the positively charged ions are the oxidized form of the electrode metal. One electrode will undergo oxidation (the anode) and the other will undergo reduction (the cathode). The metal of the anode will oxidize, going from an oxidation state of 0 (in the solid form) to a positive oxidation state and become an ion. At the cathode, the metal ion in solution will accept one or more electrons from the cathode and the ion's oxidation state is reduced to 0. This forms a solid metal that electrodeposits on the cathode. The two electrodes must be electrically connected to each other, allowing for a flow of electrons that leave the metal of the anode and flow through this connection to the ions at the surface of the cathode. This flow of electrons is an electric current that can be used to do work, such as turn a motor or power a light. A galvanic cell whose electrodes are zinc and copper submerged in zinc sulfate and copper sulfate, respectively, is known as a Daniell cell.The half reactions in a Daniell cell are as follows: Zinc electrode (anode): Zn(s) → Zn2+(aq) + 2 e− Copper electrode (cathode): Cu2+(aq) + 2 e− → Cu(s)In this example, the anode is the zinc metal which is oxidized (loses electrons) to form zinc ions in solution, and copper ions accept electrons from the copper metal electrode and the ions deposit at the copper cathode as an electrodeposit. This cell forms a simple battery as it will spontaneously generate a flow of electric current from the anode to the cathode through the external connection. This reaction can be driven in reverse by applying a voltage, resulting in the deposition of zinc metal at the anode and formation of copper ions at the cathode.To provide a complete electric circuit, there must also be an ionic conduction path between the anode and cathode electrolytes in addition to the electron conduction path. The simplest ionic conduction path is to provide a liquid junction. To avoid mixing between the two electrolytes, the liquid junction can be provided through a porous plug that allows ion flow while minimizing electrolyte mixing. To further minimize mixing of the electrolytes, a salt bridge can be used which consists of an electrolyte saturated gel in an inverted U-tube. As the negatively charged electrons flow in one direction around this circuit, the positively charged metal ions flow in the opposite direction in the electrolyte. To allow prediction of the cell potential, tabulations of standard electrode potential are available. Such tabulations are referenced to the standard hydrogen electrode (SHE). The standard hydrogen electrode undergoes the reaction  2 H+(aq) + 2 e− → H2which is shown as a reduction but, in fact, the SHE can act as either the anode or the cathode, depending on the relative oxidation/reduction potential of the other electrode/electrolyte combination. The term standard in SHE requires a supply of hydrogen gas bubbled through the electrolyte at a pressure of 1 atm and an acidic electrolyte with H+ activity equal to 1 (usually assumed to be [H+] = 1 mol/liter, i.e. pH = 0). The SHE electrode can be connected to any other electrode by a salt bridge and an external circuit to form a cell. If the second electrode is also at standard conditions, then the measured cell potential is called the standard electrode potential for the electrode. The standard electrode potential for the SHE is zero, by definition. The polarity of the standard electrode potential provides information about the relative reduction potential of the electrode compared to the SHE. If the electrode has a positive potential with respect to the SHE, then that means it is a strongly reducing electrode which forces the SHE to be the anode (an example is Cu in aqueous CuSO4 with a standard electrode potential of 0.337 V). Conversely, if the measured potential is negative, the electrode is more oxidizing than the SHE (such as Zn in ZnSO4 where the standard electrode potential is −0.76 V).Standard electrode potentials are usually tabulated as reduction potentials. However, the reactions are reversible and the role of a particular electrode in a cell depends on the relative oxidation/reduction potential of both electrodes. The oxidation potential for a particular electrode is just the negative of the reduction potential. A standard cell potential can be determined by looking up the standard electrode potentials for both electrodes (sometimes called half cell potentials). The one that is smaller will be the anode and will undergo oxidation. The cell potential is then calculated as the sum of the reduction potential for the cathode and the oxidation potential for the anode.  E°cell = E°red (cathode) – E°red (anode) = E°red (cathode) + E°oxi (anode)For example, the standard electrode potential for a copper electrode is: During operation of an electrochemical cell, chemical energy is transformed into electrical energy. This can be expressed mathematically as the product of the cell's emf Ecell measured in volts (V) and the electric charge Qele,trans transferred through the external circuit.  Electrical energy = EcellQele,transQele,trans is the cell current integrated over time and measured in coulombs (C); it can also be determined by multiplying the total number ne of electrons transferred (measured in moles) times Faraday's constant (F). The emf of the cell at zero current is the maximum possible emf. It can be used to calculate the maximum possible electrical energy that could be obtained from a chemical reaction. This energy is referred to as electrical work and is expressed by the following equation:      Many types of battery have been commercialized and represent an important practical application of electrochemistry. Early wet cells powered the first telegraph and telephone systems, and were the source of current for electroplating. The zinc-manganese dioxide dry cell was the first portable, non-spillable battery type that made flashlights and other portable devices practical.  The mercury battery using zinc and mercuric oxide provided higher levels of power and capacity than the original dry cell for early electronic devices, but has been phased out of common use due to the danger of mercury pollution from discarded cells. The lead–acid battery was the first practical secondary (rechargeable) battery that could have its capacity replenished from an external source. The electrochemical reaction that produced current was (to a useful degree) reversible, allowing electrical energy and chemical energy to be interchanged as needed. Common lead acid batteries contain a mixture of sulfuric acid and water, as well as lead plates. The most common mixture used today is 30% acid. One problem, however, is if left uncharged acid will crystallize within the lead plates of the battery rendering it useless. These batteries last an average of 3 years with daily use but it is not unheard of for a lead acid battery to still be functional after 7–10 years. Lead-acid cells continue to be widely used in automobiles. All the preceding types have water-based electrolytes, which limits the maximum voltage per cell. The freezing of water limits low temperature performance. The lithium metal battery, which does not (and cannot) use water in the electrolyte, provides improved performance over other types; a rechargeable lithium-ion battery is an essential part of many mobile devices. The flow battery, an experimental type, offers the option of vastly larger energy capacity because its reactants can be replenished from external reservoirs. The fuel cell can turn the chemical energy bound in hydrocarbon gases or hydrogen and oxygen directly into electrical energy with a much higher efficiency than any combustion process; such devices have powered many spacecraft and are being applied to grid energy storage for the public power system. Corrosion is an electrochemical process, which reveals itself as rust or tarnish on metals like iron or copper and their respective alloys, steel and brass. The spontaneous redox reactions of a conventional battery produce electricity through the different reduction potentials of the cathode and anode in the electrolyte. However, electrolysis requires an external source of electrical energy to induce a chemical reaction, and this process takes place in a compartment called an electrolytic cell. There are various important electrochemical processes in both nature and industry, like the coating of objects with metals or metal oxides through electrodeposition, the addition (electroplating) or removal (electropolishing) of thin layers of metal from an object's surface, and the detection of alcohol in drunk drivers through the redox reaction of ethanol. The generation of chemical energy through photosynthesis is inherently an electrochemical process, as is production of metals like aluminum and titanium from their ores. Certain diabetes blood sugar meters measure the amount of glucose in the blood through its redox potential. In addition to established electrochemical technologies (like deep cycle lead acid batteries) there is also a wide range of new emerging technologies such as fuel cells, large format lithium-ion batteries, electrochemical reactors and super-capacitors that are becoming increasingly commercial. Electrochemical or coulometric titrations were introduced for quantitative analysis of minute quantities in 1938 by the Hungarian chemists László Szebellédy and Zoltan Somogyi. Electrochemistry also has important applications in the food industry, like the assessment of food/package interactions, the analysis of milk composition, the characterization and the determination of the freezing end-point of ice-cream mixes, or the determination of free acidity in olive oil.   Kreysa, Gerhard; Ota, Ken-ichiro; Savinell, Robert F., eds. (2014). Encyclopedia of Applied Electrochemistry. New York, NY: Springer New York. doi:10.1007/978-1-4419-6996-5. ISBN 978-1-4419-6995-8. Ebbing, Darrell D. and Gammon, Steven D. General Chemistry (2007) ISBN 0-618-73879-7, Nobel Lectures in Chemistry, Volume 1, World Scientific (1999) ISBN 981-02-3405-8 Swaddle, Thomas Wilson Inorganic chemistry: an industrial and environmental perspective, Academic Press (1997) ISBN 0-12-678550-3 Brett CMA, Brett AMO, ELECTROCHEMISTRY, Principles, methods, and applications, Oxford University Press, (1993) ISBN 0-19-855389-7 Wiberg, Egon; Wiberg, Nils and Holleman, Arnold Frederick Inorganic chemistry, Academic Press (2001) ISBN 0-12-352651-5  Media related to Electrochemistry at Wikimedia Commons Electrochemistry at Curlie\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Non-Chemistry Documents:\n",
      "Document 1 (Artificial intelligence):\n",
      "The general problem of simulating (or creating) intelligence has been broken into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. AI research uses a wide variety of techniques to accomplish the goals above. AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). AI, like any powerful technology, has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified.Anyone looking to use machine learning as part of real-world, in-production systems needs to factor ethics into their AI training processes and strive to avoid bias. This is especially true when using AI algorithms that are inherently unexplainable in deep learning. The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate both mathematical deduction and formal reasoning, which is known as the Church–Turing thesis. This, along with concurrent discoveries in cybernetics and information theory, led researchers to consider the possibility of building an \"electronic brain\".Alan Turing was thinking about machine intelligence at least as early as 1941, when he circulated a paper on machine intelligence which could be the earliest paper in the field of AI – though it is now lost. The first available paper generally recognized as \"AI\" was McCullouch and Pitts design for Turing-complete \"artificial neurons\" in 1943 – the first mathematical model of a neural network. The paper was influenced by Turing's earlier paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function.The term 'Machine Intelligence' was used by Alan Turing during his life which was later often referred to as 'Artificial Intelligence' after his death in 1954. In 1950 Turing published the best known of his papers 'Computing Machinery and Intelligence', the paper introduced his concept of what is now known as the Turing test to the general public. Then followed three radio broadcasts on AI by Turing, the lectures: 'Intelligent Machinery, A Heretical Theory’, ‘Can Digital Computers Think’? and the panel discussion ‘Can Automatic Calculating Machines be Said to Think’. By 1956 computer intelligence had been actively pursued for more than a decade in Britain; the earliest AI programmes were written there in 1951–1952.In 1951, using a Ferranti Mark 1 computer of the University of Manchester, checkers and chess programs were wrote where you could play against the computer. The field of American AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial Intelligence laboratories were set up at a number of British and US Universities in the latter 1950s and early 1960s.They had, however, underestimated the difficulty of the problem. Both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.Many researchers began to doubt that the current practices would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches. Robotics researchers, such as Rodney Brooks, rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\".Several academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field. For many specific tasks, other methods were abandoned. Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.   Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. AI effect Artificial intelligence detection software – Software to detect AI-generated contentPages displaying short descriptions of redirect targets Artificial intelligence in healthcare – Overview of the use of artificial intelligence in healthcare Behavior selection algorithm – Algorithm that selects actions for intelligent agents Business process automation – Technology-enabled automation of complex business processes Case-based reasoning – Process of solving new problems based on the solutions of similar past problems     \"Artificial Intelligence\". Internet Encyclopedia of Philosophy. Thomason, Richmond. \"Logic and Artificial Intelligence\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy. Artificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005). Theranostics and AI—The Next Advance in Cancer Precision Medicine.\n",
      "\n",
      "---\n",
      "\n",
      "Document 2 (Computer Science):\n",
      "The earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.Wilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\". \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1914, the Spanish engineer Leonardo Torres Quevedo published his Essays on Automatics, and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic. In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, a prototype that demonstrated the feasibility of an electromechanical analytical engine, on which commands could be typed and the results printed automatically. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\". During the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights. Although first proposed in 1956, the term \"computer science\" appears in a 1959 article in Communications of the ACM, in which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline. His efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases. In the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic. Computer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.The relationship between computer science and software engineering is a contentious issue, which is further muddied by disputes over what the term \"software engineering\" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.  As a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science. The philosopher of computing Bill Rapaport noted three Great Insights of Computer Science: Gottfried Wilhelm Leibniz's, George Boole's, Alan Turing's, Claude Shannon's, and Samuel Morse's insight: there are only two objects that a computer has to deal with in order to represent \"anything\".All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as \"on/off\", \"magnetized/de-magnetized\", \"high-voltage/low-voltage\", etc.). Alan Turing's insight: there are only five actions that a computer has to perform in order to do \"anything\".Every algorithm can be expressed in a language for a computer consisting of only five basic instructions:move left one location; move right one location; read symbol at current location; print 0 at current location; Programming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:  Functional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements. Imperative programming, a programming paradigm that uses statements that change a program's state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates. Object-oriented programming, a programming paradigm based on the concept of \"objects\", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object's procedures can access and often modify the data fields of the object with which they are associated. Thus object-oriented computer programs are made out of objects that interact with one another. Service-oriented programming, a programming paradigm that uses \"services\" as the unit of computer work, to design and implement integrated business applications and mission critical software programsMany languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities. Conferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals. Computer Science, known by its near synonyms, Computing, Computer Studies, has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students. In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11–16-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all pupils over the age of 4.In the US, with 14,000 school districts deciding the curriculum, provision was fractured. According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science. According to a 2021 report, only 51% of high schools in the US offer computer science.Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula, and several others are following.      DBLP Computer Science Bibliography Association for Computing Machinery Institute of Electrical and Electronics Engineers\n",
      "\n",
      "---\n",
      "\n",
      "Document 3 (Medical):\n",
      "Medicine (UK:  , US:  ) is the science and practice of the diagnosis, prognosis, treatment, and prevention of disease. The word \"medicine\" is derived from Latin medicus, meaning \"a physician\". Medical availability and clinical practice vary across the world due to regional differences in culture and technology. Modern scientific medicine is highly developed in the Western world, while in developing countries such as parts of Africa or Asia, the population may rely more heavily on traditional medicine with limited evidence and efficacy and no required formal training for practitioners.In the developed world, evidence-based medicine is not universally used in clinical practice; for example, a 2007 survey of literature reviews found that about 49% of the interventions lacked sufficient evidence to support either benefit or harm.In modern clinical practice, physicians and physician assistants personally assess patients to diagnose, prognose, treat, and prevent disease using clinical judgment. The doctor-patient relationship typically begins with an interaction with an examination of the patient's medical history and medical record, followed by a medical interview and a physical examination. Basic diagnostic medical devices (e.g., stethoscope, tongue depressor) are typically used. After examining for signs and interviewing for symptoms, the doctor may order medical tests (e.g., blood tests), take a biopsy, or prescribe pharmaceutical drugs or other therapies. Differential diagnosis methods help to rule out conditions based on the information provided. During the encounter, properly informing the patient of all relevant facts is an important part of the relationship and the development of trust. The medical encounter is then documented in the medical record, which is a legal document in many jurisdictions. Follow-ups may be shorter but follow the same general procedure, and specialists follow a similar process. The diagnosis and treatment may take only a few minutes or a few weeks, depending on the complexity of the issue. The components of the medical interview and encounter are:  Chief complaint (CC): the reason for the current medical visit. These are the symptoms. They are in the patient's own words and are recorded along with the duration of each one. Also called chief concern or presenting complaint. Current activity: occupation, hobbies, what the patient actually does. Family history (FH): listing of diseases in the family that may impact the patient. A family tree is sometimes used. Contemporary medicine is, in general, conducted within health care systems. Legal, credentialing, and financing frameworks are established by individual governments, augmented on occasion by international organizations, such as churches. The characteristics of any given health care system have a significant impact on the way medical care is provided. From ancient times, Christian emphasis on practical charity gave rise to the development of systematic nursing and hospitals, and the Catholic Church today remains the largest non-government provider of medical services in the world. Advanced industrial countries (with the exception of the United States) and many developing countries provide medical services through a system of universal health care that aims to guarantee care for all through a single-payer health care system or compulsory private or cooperative health insurance. This is intended to ensure that the entire population has access to medical care on the basis of need rather than ability to pay. Delivery may be via private medical practices, state-owned hospitals and clinics, or charities, most commonly a combination of all three. Most tribal societies provide no guarantee of healthcare for the population as a whole. In such societies, healthcare is available to those who can afford to pay for it, have self-insured it (either directly or as part of an employment contract), or may be covered by care financed directly by the government or tribe.  Transparency of information is another factor defining a delivery system. Access to information on conditions, treatments, quality, and pricing greatly affects the choice of patients/consumers and, therefore, the incentives of medical professionals. While the US healthcare system has come under fire for its lack of openness, new legislation may encourage greater openness. There is a perceived tension between the need for transparency on the one hand and such issues as patient confidentiality and the possible exploitation of information for commercial gain on the other. The health professionals who provide care in medicine comprise multiple professions, such as medics, nurses, physiotherapists, and psychologists. These professions will have their own ethical standards, professional education, and bodies. The medical profession has been conceptualized from a sociological perspective. Working together as an interdisciplinary team, many highly trained health professionals besides medical practitioners are involved in the delivery of modern health care. Examples include: nurses, emergency medical technicians and paramedics, laboratory scientists, pharmacists, podiatrists, physiotherapists, respiratory therapists, speech therapists, occupational therapists, radiographers, dietitians, and bioengineers, medical physicists, surgeons, surgeon's assistant, surgical technologist. The scope and sciences underpinning human medicine overlap many other fields. A patient admitted to the hospital is usually under the care of a specific team based on their main presenting problem, e.g., the cardiology team, who then may interact with other specialties, e.g., surgical, radiology, to help diagnose or treat the main problem or any subsequent complications/developments. Physicians have many specializations and subspecializations into certain branches of medicine, which are listed below. There are variations from country to country regarding which specialties certain subspecialties are in. The main branches of medicine are:  Basic sciences of medicine; this is what every physician is educated in, and some return to in biomedical research. Medical education and training varies around the world. It typically involves entry level education at a university medical school, followed by a period of supervised practice or internship, or residency. This can be followed by postgraduate vocational training. A variety of teaching methods have been employed in medical education, still itself a focus of active research. In Canada and the United States of America, a Doctor of Medicine degree, often abbreviated M.D., or a Doctor of Osteopathic Medicine degree, often abbreviated as D.O. and unique to the United States, must be completed in and delivered from a recognized university. Since knowledge, techniques, and medical technology continue to evolve at a rapid rate, many regulatory authorities require continuing medical education. Medical practitioners upgrade their knowledge in various ways, including medical journals, seminars, conferences, and online programs.  A database of objectives covering medical knowledge, as suggested by national societies across the United States, can be searched at http://data.medobjectives.marian.edu/ Archived 4 October 2018 at the Wayback Machine. In most countries, it is a legal requirement for a medical doctor to be licensed or registered. In general, this entails a medical degree from a university and accreditation by a medical board or an equivalent national organization, which may ask the applicant to pass exams. This restricts the considerable legal authority of the medical profession to physicians that are trained and qualified by national standards. It is also intended as an assurance to patients and as a safeguard against charlatans that practice inadequate medicine for personal gain. While the laws generally require medical doctors to be trained in \"evidence based\", Western, or Hippocratic Medicine, they are not intended to discourage different paradigms of health. In the European Union, the profession of doctor of medicine is regulated. A profession is said to be regulated when access and exercise is subject to the possession of a specific professional qualification. The regulated professions database contains a list of regulated professions for doctor of medicine in the EU member states, EEA countries and Switzerland. This list is covered by the Directive 2005/36/EC. Doctors who are negligent or intentionally harmful in their care of patients can face charges of medical malpractice and be subject to civil, criminal, or professional sanctions. Medical ethics is a system of moral principles that apply values and judgments to the practice of medicine. As a scholarly discipline, medical ethics encompasses its practical application in clinical settings as well as work on its history, philosophy, theology, and sociology. Six of the values that commonly apply to medical ethics discussions are:  autonomy – the patient has the right to refuse or choose their treatment. (Latin: Voluntas aegroti suprema lex.) beneficence – a practitioner should act in the best interest of the patient. (Latin: Salus aegroti suprema lex.) justice – concerns the distribution of scarce health resources, and the decision of who gets what treatment (fairness and equality). non-maleficence – \"first, do no harm\" (Latin: primum non-nocere).  Evidence-based medicine, prevention of medical error (and other \"iatrogenesis\"), and avoidance of unnecessary health care are a priority in modern medical systems. These topics generate significant political and public policy attention, particularly in the United States where healthcare is regarded as excessively costly but population health metrics lag similar nations.Globally, many developing countries lack access to care and access to medicines. As of 2015, most wealthy developed countries provide health care to all citizens, with a few exceptions such as the United States where lack of health insurance coverage may limit access.   == References ==\n",
      "\n",
      "---\n",
      "\n",
      "Document 4 (History):\n",
      "The word history comes from historía (Ancient Greek: ἱστορία, romanized: historíā, lit. 'inquiry, knowledge from inquiry, or judge'). It was in that sense that Aristotle used the word in his History of Animals. The ancestor word ἵστωρ is attested early on in Homeric Hymns, Heraclitus, the Athenian ephebes' oath, and in Boeotic inscriptions (in a legal sense, either \"judge\" or \"witness\", or similar). The Greek word was borrowed into Classical Latin as historia, meaning \"investigation, inquiry, research, account, description, written account of past events, writing of history, historical narrative, recorded knowledge of past events, story, narrative\". History was borrowed from Latin (possibly via Old Irish or Old Welsh) into Old English as stær (\"history, narrative, story\"), but this word fell out of use in the late Old English period. Meanwhile, as Latin became Old French (and Anglo-Norman), historia developed into forms such as istorie, estoire, and historie, with new developments in the meaning: \"account of the events of a person's life (beginning of the 12th century), chronicle, account of events as relevant to a group of people or people in general (1155), dramatic or pictorial representation of historical events (c. 1240), body of knowledge relative to human evolution, science (c. 1265), narrative of real or imaginary events, story (c. 1462)\".It was from Anglo-Norman that history was brought into Middle English, and it has persisted. It appears in the 13th-century Ancrene Wisse, but seems to have become a common word in the late 14th century, with an early attestation appearing in John Gower's Confessio Amantis of the 1390s (VI.1383): \"I finde in a bok compiled | To this matiere an old histoire, | The which comth nou to mi memoire\". In Middle English, the meaning of history was \"story\" in general. The restriction to the meaning \"the branch of knowledge that deals with past events; the formal record or study of past events, esp. human affairs\" arose in the mid-15th century. With the Renaissance, older senses of the word were revived, and it was in the Greek sense that Francis Bacon used the term in the late 16th century, when he wrote about natural history. For him, historia was \"the knowledge of objects determined by space and time\", that sort of knowledge provided by memory (while science was provided by reason, and poetry was provided by fantasy).In an expression of the linguistic synthetic vs. analytic/isolating dichotomy, English like Chinese (史 vs. 诌) now designates separate words for human history and storytelling in general. In modern German, French, and most Germanic and Romance languages, which are solidly synthetic and highly inflected, the same word is still used to mean both \"history\" and \"story\". Historian in the sense of a \"researcher of history\" is attested from 1531. In all European languages, the substantive history is still used to mean both \"what happened with men\", and \"the scholarly study of the happened\", the latter sense sometimes distinguished with a capital letter, or the word historiography. The adjective historical is attested from 1661, and historic from 1669. Historians write in the context of their own time, and with due regard to the current dominant ideas of how to interpret the past, and sometimes write to provide lessons for their own society. In the words of Benedetto Croce, \"All history is contemporary history\". History is facilitated by the formation of a \"true discourse of past\" through the production of narrative and analysis of past events relating to the human race. The modern discipline of history is dedicated to the institutional production of this discourse. All events that are remembered and preserved in some authentic form constitute the historical record. The task of historical discourse is to identify the sources which can most usefully contribute to the production of accurate accounts of past. Therefore, the constitution of the historian's archive is a result of circumscribing a more general archive by invalidating the usage of certain texts and documents (by falsifying their claims to represent the \"true past\"). Part of the historian's role is to skillfully and objectively use the many sources from the past, most often found in the archives. The process of creating a narrative inevitably generates debate, as historians remember or emphasize different events of the past.The study of history has sometimes been classified as part of the humanities, other times part of the social sciences. It can be seen as a bridge between those two broad areas, incorporating methodologies from both. Some historians strongly support one or the other classification. In the 20th century the Annales school revolutionized the study of history, by using such outside disciplines as economics, sociology, and geography in the study of global history.Traditionally, historians have recorded events of the past, either in writing or by passing on an oral tradition, and attempted to answer historical questions through the study of written documents and oral accounts. From the beginning, historians have used such sources as monuments, inscriptions, and pictures. In general, the sources of historical knowledge can be separated into three categories: what is written, what is said, and what is physically preserved, and historians often consult all three. But writing is the marker that separates history from what comes before. Archaeology is especially helpful in unearthing buried sites and objects, which contribute to the study of history. Archeological finds rarely stand alone, with narrative sources complementing its discoveries. Archeology's methodologies and approaches are independent from the field of history. \"Historical archaeology\" is a specific branch of archeology which often contrasts its conclusions against those of contemporary textual sources. For example, Mark Leone, the excavator and interpreter of historical Annapolis, Maryland, US, has sought to understand the contradiction between textual documents idealizing \"liberty\" and the material record, demonstrating the possession of slaves and the inequalities of wealth made apparent by the study of the total historical environment. There are varieties of ways in which history can be organized, including chronologically, culturally, territorially, and thematically. These divisions are not mutually exclusive, and significant intersections are present. It is possible for historians to concern themselves with both the very specific and the very general, though the trend has been toward specialization. The area called Big History resists this specialization, and searches for universal patterns or trends. History has often been studied with some practical or theoretical aim, but may be studied out of simple intellectual curiosity. Human history is the memory of the past experience of Homo sapiens sapiens around the world, as that experience has been preserved, largely in written records. By \"prehistory\", historians mean the recovery of knowledge of the past in an area where no written records exist, or where the writing of a culture is not understood. By studying painting, drawings, carvings, and other artifacts, some information can be recovered even in the absence of a written record. Since the 20th century, the study of prehistory is considered essential to avoid history's implicit exclusion of certain civilizations, such as those of sub-Saharan Africa and pre-Columbian America. Historians in the West have been criticized for focusing disproportionately on the Western world. In 1961, British historian E. H. Carr wrote:  The line of demarcation between prehistoric and historical times is crossed when people cease to live only in the present, and become consciously interested both in their past and in their future. History begins with the handing down of tradition; and tradition means the carrying of the habits and lessons of the past into the future. Records of the past begin to be kept for the benefit of future generations. This definition includes within the scope of history the strong interests of peoples, such as Indigenous Australians and New Zealand Māori in the past, and the oral records maintained and transmitted to succeeding generations, even before their contact with European civilization. Historiography has a number of related meanings. Firstly, it can refer to how history has been produced: the story of the development of methodology and practices (for example, the move from short-term biographical narrative toward long-term thematic analysis). Secondly, it can refer to what has been produced: a specific body of historical writing (for example, \"medieval historiography during the 1960s\" means \"Works of medieval history written during the 1960s\"). Thirdly, it may refer to why history is produced: the philosophy of history. As a meta-level analysis of descriptions of the past, this third conception can relate to the first two in that the analysis usually focuses on the narratives, interpretations, world view, use of evidence, or method of presentation of other historians. Historians debate whether history can be taught as a single coherent narrative or a series of competing narratives. Europeans have written and published extensively to pull together a \"universal history\" in the early modern period. This written corpus and discourse in Europe includes ethnographic encounters, comparative philosophy, as well as archaeological discovery.Herodotus, from the 5th-century BC, has been acclaimed as the \"father of history\". However, his contemporary Thucydides is credited with having first approached history with a well-developed historical method in the History of the Peloponnesian War. Thucydides, unlike Herodotus, regarded history as the product of the choices and actions of humans, and looked at cause and effect, rather than the result of divine intervention (though Herodotus was not wholly committed to this idea himself). In his historical method, Thucydides emphasized chronology, a nominally neutral point of view, and that the human world was the result of human actions. Greek historians viewed history as cyclical, with events regularly recurring.There was sophisticated use of historical method in ancient and medieval China. The groundwork for professional historiography in East Asia was established by court historian Sima Qian (145–90 BC), author of the Records of the Grand Historian (Shiji) and posthumously known as the Father of Chinese historiography. Saint Augustine was influential in Christian and Western thought at the beginning of the medieval period. Through the Medieval and Renaissance periods, history was often studied through a sacred or religious perspective. Around 1800, German philosopher and historian Georg Wilhelm Friedrich Hegel brought philosophy and a more secular approach in historical study.In the preface to his book, the Muqaddimah (1377), the Arab historian and early sociologist, Ibn Khaldun, warned of 7 mistakes he thought historians committed. In this criticism, he approached the past as strange and in need of interpretation. The originality of Ibn Khaldun was to claim that the cultural difference of another age must govern the evaluation of relevant historical material, to distinguish the principles according to which it might be possible to attempt the evaluation, and to feel the need for experience, in addition to rational principles, in order to assess a culture of the past. Ibn Khaldun criticized \"idle superstition and uncritical acceptance of historical data\". He introduced a scientific method to the study of history, and referred to it as his \"new science\". His method laid the groundwork for the observation of the role of state, communication, propaganda and systematic bias in history, and so is considered to be the \"father of historiography\" or the \"father of the philosophy of history\".In the West, historians developed modern methods of historiography in the 17th and 18th centuries, especially in France and Germany. In 1851, Herbert Spencer summarized these methods:\"From the successive strata of our historical deposits, they [historians] diligently gather all the highly colored fragments, pounce upon everything that is curious and sparkling and chuckle like children over their glittering acquisitions; meanwhile the rich veins of wisdom that ramify amidst this worthless debris, lie utterly neglected. Cumbrous volumes of rubbish are greedily accumulated, while those masses of rich ore, that should have been dug out, and from which golden truths might have been smelted, are left untaught and unsought.\" By the \"rich ore\" Spencer meant scientific theory of history. Meanwhile, Henry Thomas Buckle expressed a dream of history becoming one day a science: \"In regard to nature, events apparently the most irregular and capricious have been explained and have been shown to be in accordance with certain fixed and universal laws. This has been done because men of ability and, above all, men of patient, untiring thought have studied events with the view of discovering their regularity, and if human events were subject to a similar treatment, we have every right to expect similar results. Contrary to Buckle's dream, the 19th-century historian with greatest influence on methods became Leopold von Ranke in Germany. He limited history to \"what really happened\" and by this directed the field further away from science. For Ranke, historical data should be collected carefully, examined objectively and put together with critical rigor. But these procedures \"are merely the prerequisites and preliminaries of science. The heart of science is searching out order and regularity in the data being examined and in formulating generalizations or laws about them.\"  As Historians like Ranke and many who followed him have pursued it, no, history is not a science. Thus if Historians tell us that, given the manner in which he practices his craft, it cannot be considered a science, we must take him at his word. If he is not doing science, then, whatever else he is doing, he is not doing science. The traditional Historian is thus no scientist and history, as conventionally practiced, is not a science. In the 20th century, academic historians focused less on epic nationalistic narratives, which often tended to glorify the nation or great men, to more objective and complex analyses of social and intellectual forces. A major trend of historical methodology in the 20th century was to treat history more as a social science rather than art, which traditionally had been the case. Leading advocates of history as a social science were a diverse collection of scholars which included Fernand Braudel and E. H. Carr. Many are noted for their multidisciplinary approach e.g. Braudel combined history with geography. Nevertheless, these multidisciplinary approaches failed to produce a theory of history. So far only one theory of history came from a professional historian. Whatever other theories of history exist, they were written by experts from other fields (for example, Marxian theory of history). The field of digital history has begun to address ways of using computer technology, to pose new questions to historical data and generate digital scholarship.  Professional and amateur historians discover, collect, organize, and present information about past events. They discover this information through archeological evidence, written primary sources, verbal stories or oral histories, and other archival material. In lists of historians, historians can be grouped by order of the historical period in which they were writing, which is not necessarily the same as the period in which they specialized. Chroniclers and annalists, though they are not historians in the true sense, are also frequently included. Since the 20th century, Western historians have disavowed the aspiration to provide the \"judgement of history\". The goals of historical judgements or interpretations are separate to those of legal judgements, that need to be formulated quickly after the events and be final. A related issue to that of the judgement of history is that of collective memory. Pseudohistory is a term applied to texts which purport to be historical in nature but which depart from standard historiographical conventions in a way which undermines their conclusions. It is closely related to deceptive historical revisionism. Works which draw controversial conclusions from new, speculative, or disputed historical evidence, particularly in the fields of national, political, military, and religious affairs, are often rejected as pseudohistory.  Glossary of history Outline of history   Official website of BestHistorySites Official website of BBC History Internet History Sourcebooks Project See also Internet History Sourcebooks Project (Collections of public domain and copy-permitted historical texts for educational use)\n",
      "\n",
      "---\n",
      "\n",
      "Document 5 (Astronomy):\n",
      "Astronomy (from the Greek ἀστρονομία from ἄστρον astron, \"star\" and -νομία -nomia from νόμος nomos, \"law\" or \"culture\") means \"law of the stars\" (or \"culture of the stars\" depending on the translation). Astronomy should not be confused with astrology, the belief system which claims that human affairs are correlated with the positions of celestial objects. Although the two fields share a common origin, they are now entirely distinct.  The main source of information about celestial bodies and other objects is visible light, or more generally electromagnetic radiation. Observational astronomy may be categorized according to the corresponding region of the electromagnetic spectrum on which the observations are made. Some parts of the spectrum can be observed from the Earth's surface, while other parts are only observable from either high altitudes or outside the Earth's atmosphere. Specific information on these subfields is given below. Theoretical astronomers use several tools including analytical models and computational numerical simulations; each has its particular advantages. Analytical models of a process are better for giving broader insight into the heart of what is going on. Numerical models reveal the existence of phenomena and effects otherwise unobserved.Theorists in astronomy endeavor to create theoretical models that are based on existing observations and known physics, and to predict observational consequences of those models. The observation of phenomena predicted by a model allows astronomers to select between several alternative or conflicting models. Theorists also modify existing models to take into account new observations. In some cases, a large amount of observational data that is inconsistent with a model may lead to abandoning it largely or completely, as for geocentric theory, the existence of luminiferous aether, and the steady-state model of cosmic evolution. Phenomena modeled by theoretical astronomers include:  stellar dynamics and evolution galaxy formation large-scale distribution of matter in the Universe  Astronomy and astrophysics have developed significant interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, utilizing archaeological and anthropological evidence. Astrobiology is the study of the advent and evolution of biological systems in the Universe, with particular emphasis on the possibility of non-terrestrial life. Astrostatistics is the application of statistics to astrophysics to the analysis of a vast amount of observational astrophysical data.The study of chemicals found in space, including their formation, interaction and destruction, is called astrochemistry. These substances are usually found in molecular clouds, although they may also appear in low-temperature stars, brown dwarfs and planets. Cosmochemistry is the study of the chemicals found within the Solar System, including the origins of the elements and variations in the isotope ratios. Both of these fields represent an overlap of the disciplines of astronomy and chemistry. As \"forensic astronomy\", finally, methods from astronomy have been used to solve problems of art history and occasionally of law. Astronomy is one of the sciences to which amateurs can contribute the most.Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with consumer-level equipment or equipment that they build themselves. Common targets of amateur astronomers include the Sun, the Moon, planets, stars, comets, meteor showers, and a variety of deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs are located throughout the world and many have programs to help their members set up and complete observational programs including those to observe all the objects in the Messier (110 objects) or Herschel 400 catalogues of points of interest in the night sky. One branch of amateur astronomy, astrophotography, involves the taking of photos of the night sky. Many amateurs like to specialize in the observation of particular objects, types of objects, or types of events that interest them.Most amateurs work at visible wavelengths, but many experiment with wavelengths outside the visible spectrum. This includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. The pioneer of amateur radio astronomy was Karl Jansky, who started observing the sky at radio wavelengths in the 1930s. A number of amateur astronomers use either homemade telescopes or use radio telescopes which were originally built for astronomy research but which are now available to amateurs (e.g. the One-Mile Telescope).Amateur astronomers continue to make scientific contributions to the field of astronomy and it is one of the few scientific disciplines where amateurs can still make significant contributions. Amateurs can make occultation measurements that are used to refine the orbits of minor planets. They can also discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make impressive advances in the field of astrophotography. In the 21st century there remain important unanswered questions in astronomy. Some are cosmic in scope: for example, what are dark matter and dark energy? These dominate the evolution and fate of the cosmos, yet their true nature remains unknown. What will be the ultimate fate of the universe? Why is the abundance of lithium in the cosmos four times lower than predicted by the standard Big Bang model? Others pertain to more specific classes of phenomena. For example, is the Solar System normal or atypical? What is the origin of the stellar mass spectrum? That is, why do astronomers observe the same distribution of stellar masses—the initial mass function—apparently regardless of the initial conditions? Likewise, questions remain about the formation of the first galaxies,  the origin of supermassive black holes, the source of ultra-high-energy cosmic rays, and more. Is there other life in the Universe? Especially, is there other intelligent life? If so, what is the explanation for the Fermi paradox? The existence of life elsewhere has important scientific and philosophical implications. Cosmogony – Branch of science or a theory concerning the origin of the universe Outline of astronomy Outline of space science – Overview of and topical guide to space science Space exploration – Exploration of space, planets, and moons  Newcomb, Simon; Clerke, Agnes Mary (1911). \"Astronomy\" . Encyclopædia Britannica. Vol. 2 (11th ed.). pp. 800–819. Harpaz, Amos (1994). Stellar Evolution. A K Peters, Ltd. ISBN 978-1-56881-012-6. Unsöld, A.; Baschek, B. (2001). The New Cosmos: An Introduction to Astronomy and Astrophysics. Springer. ISBN 978-3-540-67877-9. James, C. Renée (2023). Things That Go Bump in the Universe: How Astronomers Decode Cosmic Chaos. Johns Hopkins University Press. ISBN 978-1421446936. Archived from the original on 13 December 2023.  NASA/IPAC Extragalactic Database (NED) (NED-Distances) Core books and Core journals in Astronomy, from the Smithsonian/NASA Astrophysics Data System\n",
      "\n",
      "---\n",
      "\n",
      "Document 6 (Earth):\n",
      "The Modern English word  Earth developed, via Middle English, from an Old English noun most often spelled eorðe. It has cognates in every Germanic language, and their ancestral root has been reconstructed as *erþō. In its earliest attestation, the word eorðe was used to translate the many senses of Latin terra and Greek γῆ gē: the ground, its soil, dry land, the human world, the surface of the world (including the sea), and the globe itself. As with Roman Terra/Tellūs and Greek Gaia, Earth may have been a personified goddess in Germanic paganism: late Norse mythology included Jörð (\"Earth\"), a giantess often given as the mother of Thor.Historically, \"earth\" has been written in lowercase. Beginning with the use of Early Middle English, its definite sense as \"the globe\" was expressed as \"the earth\". By the era of Early Modern English, capitalization of nouns began to prevail, and the earth was also written the Earth, particularly when referenced along with other heavenly bodies. More recently, the name is sometimes simply given as Earth, by analogy with the names of the other planets, though \"earth\" and forms with \"the earth\" remain common. House styles now vary: Oxford spelling recognizes the lowercase form as the most common, with the capitalized form an acceptable variant. Another convention capitalizes \"Earth\" when appearing as a name, such as a description of the \"Earth's atmosphere\", but employs the lowercase when it is preceded by \"the\", such as \"the atmosphere of the earth\"). It almost always appears in lowercase in colloquial expressions such as \"what on earth are you doing?\"The name Terra  occasionally is used in scientific writing and especially in science fiction to distinguish humanity's inhabited planet from others, while in poetry Tellus  has been used to denote personification of the Earth. Terra is also the name of the planet in some Romance languages, languages that evolved from Latin, like Italian and Portuguese, while in other Romance languages the word gave rise to names with slightly altered spellings, like the Spanish Tierra and the French Terre. The Latinate form Gæa or Gaea (English: ) of the Greek poetic name Gaia (Γαῖα; Ancient Greek: [ɡâi̯.a] or [ɡâj.ja]) is rare, though the alternative spelling Gaia has become common due to the Gaia hypothesis, in which case its pronunciation is  rather than the more classical English .There are a number of adjectives for the planet Earth. The word \"earthly\" is derived from \"Earth\". From the Latin Terra comes terran , terrestrial , and (via French) terrene , and from the Latin Tellus comes tellurian  and telluric.     Earth's hydrosphere is the sum of Earth's water and its distribution. Most of Earth's hydrosphere consists of Earth's global ocean. Earth's hydrosphere also consists of water in the atmosphere and on land, including clouds, inland seas, lakes, rivers, and underground waters down to a depth of 2,000 m (6,600 ft). The mass of the oceans is approximately 1.35×1018 metric tons or about 1/4400 of Earth's total mass. The oceans cover an area of 361.8 million km2 (139.7 million sq mi) with a mean depth of 3,682 m (12,080 ft), resulting in an estimated volume of 1.332 billion km3 (320 million cu mi). If all of Earth's crustal surface were at the same elevation as a smooth sphere, the depth of the resulting world ocean would be 2.7 to 2.8 km (1.68 to 1.74 mi). About 97.5% of the water is saline; the remaining 2.5% is fresh water. Most fresh water, about 68.7%, is present as ice in ice caps and glaciers. The remaining 30% is ground water, 1% surface water (covering only 2.8% of Earth's land) and other small forms of fresh water deposits such as permafrost, water vapor in the atmosphere, biological binding, etc. .In Earth's coldest regions, snow survives over the summer and changes into ice. This accumulated snow and ice eventually forms into glaciers, bodies of ice that flow under the influence of their own gravity. Alpine glaciers form in mountainous areas, whereas vast ice sheets form over land in polar regions. The flow of glaciers erodes the surface changing it dramatically, with the formation of U-shaped valleys and other landforms. Sea ice in the Arctic covers an area about as big as the United States, although it is quickly retreating as a consequence of climate change.The average salinity of Earth's oceans is about 35 grams of salt per kilogram of seawater (3.5% salt). Most of this salt was released from volcanic activity or extracted from cool igneous rocks. The oceans are also a reservoir of dissolved atmospheric gases, which are essential for the survival of many aquatic life forms. Sea water has an important influence on the world's climate, with the oceans acting as a large heat reservoir. Shifts in the oceanic temperature distribution can cause significant weather shifts, such as the El Niño–Southern Oscillation.The abundance of water, particularly liquid water, on Earth's surface is a unique feature that distinguishes it from other planets in the Solar System. Solar System planets with considerable atmospheres do partly host atmospheric water vapor, but they lack surface conditions for stable surface water. Despite some moons showing signs of large reservoirs of extraterrestrial liquid water, with possibly even more volume than Earth's ocean, all of them are large bodies of water under a kilometers thick frozen surface layer. The atmospheric pressure at Earth's sea level averages 101.325 kPa (14.696 psi), with a scale height of about 8.5 km (5.3 mi). A dry atmosphere is composed of 78.084% nitrogen, 20.946% oxygen, 0.934% argon, and trace amounts of carbon dioxide and other gaseous molecules. Water vapor content varies between 0.01% and 4% but averages about 1%. Clouds cover around two thirds of Earth's surface, more so over oceans than land. The height of the troposphere varies with latitude, ranging between 8 km (5 mi) at the poles to 17 km (11 mi) at the equator, with some variation resulting from weather and seasonal factors.Earth's biosphere has significantly altered its atmosphere. Oxygenic photosynthesis evolved 2.7 Gya, forming the primarily nitrogen–oxygen atmosphere of today. This change enabled the proliferation of aerobic organisms and, indirectly, the formation of the ozone layer due to the subsequent conversion of atmospheric O2 into O3. The ozone layer blocks ultraviolet solar radiation, permitting life on land. Other atmospheric functions important to life include transporting water vapor, providing useful gases, causing small meteors to burn up before they strike the surface, and moderating temperature. This last phenomenon is the greenhouse effect: trace molecules within the atmosphere serve to capture thermal energy emitted from the surface, thereby raising the average temperature. Water vapor, carbon dioxide, methane, nitrous oxide, and ozone are the primary greenhouse gases in the atmosphere. Without this heat-retention effect, the average surface temperature would be −18 °C (0 °F), in contrast to the current +15 °C (59 °F), and life on Earth probably would not exist in its current form. Earth is the only known place that has ever been habitable for life. Earth's life developed in Earth's early bodies of water some hundred million years after Earth formed. Earth's life has been shaping and inhabiting many particular ecosystems on Earth and has eventually expanded globally forming an overarching biosphere. Therefore, life has impacted Earth, significantly altering Earth's atmosphere and surface over long periods of time, causing changes like the Great Oxidation Event.Earth's life has over time greatly diversified, allowing the biosphere to have different biomes, which are inhabited by comparatively similar plants and animals. The different biomes developed at distinct elevations or water depths, planetary temperature latitudes and on land also with different humidity. Earth's species diversity and biomass reaches a peak in shallow waters and with forests, particularly in equatorial, warm and humid conditions. While freezing polar regions and high altitudes, or extremely arid areas are relatively barren of plant and animal life.Earth provides liquid water—an environment where complex organic molecules can assemble and interact, and sufficient energy to sustain a metabolism. Plants and other organisms take up nutrients from water, soils and the atmosphere. These nutrients are constantly recycled between different species.Extreme weather, such as tropical cyclones (including hurricanes and typhoons), occurs over most of Earth's surface and has a large impact on life in those areas. From 1980 to 2000, these events caused an average of 11,800 human deaths per year. Many places are subject to earthquakes, landslides, tsunamis, volcanic eruptions, tornadoes, blizzards, floods, droughts, wildfires, and other calamities and disasters. Human impact is felt in many areas due to pollution of the air and water, acid rain, loss of vegetation (overgrazing, deforestation, desertification), loss of wildlife, species extinction, soil degradation, soil depletion and erosion. Human activities release greenhouse gases into the atmosphere which cause global warming. This is driving changes such as the melting of glaciers and ice sheets, a global rise in average sea levels, increased risk of drought and wildfires, and migration of species to colder areas. Originating from earlier primates in Eastern Africa 300,000 years ago humans have since been migrating and with the advent of agriculture in the 10th millennium BC increasingly settling Earth's land. In the 20th century Antarctica had been the last continent to see a first and until today limited human presence. Human population has since the 19th century grown exponentially to seven billion in the early 2010s, and is projected to peak at around ten billion in the second half of the 21st century. Most of the growth is expected to take place in sub-Saharan Africa.Distribution and density of human population varies greatly around the world with the majority living in south to eastern Asia and 90% inhabiting only the Northern Hemisphere of Earth, partly due to the hemispherical predominance of the world's land mass, with 68% of the world's land mass being in the Northern Hemisphere. Furthermore, since the 19th century humans have increasingly converged into urban areas with the majority living in urban areas by the 21st century.Beyond Earth's surface humans have lived on a temporary basis, with only special purpose deep underground and underwater presence, and a few space stations. Human population virtually completely remains on Earth's surface, fully depending on Earth and the environment it sustains. Since the second half of the 20th century, some hundreds of humans have temporarily stayed beyond Earth, a tiny fraction of whom have reached another celestial body, the Moon.Earth has been subject to extensive human settlement, and humans have developed diverse societies and cultures. Most of Earth's land has been territorially claimed since the 19th century by sovereign states (countries) separated by political borders, and 205 such states exist today, with only parts of Antarctica and few small regions remaining unclaimed. Most of these states together form the United Nations, the leading worldwide intergovernmental organization, which extends human governance over the ocean and Antarctica, and therefore all of Earth. Human cultures have developed many views of the planet. The standard astronomical symbols of Earth are a quartered circle, , representing the four corners of the world, and a globus cruciger, . Earth is sometimes personified as a deity. In many cultures it is a mother goddess that is also the primary fertility deity. Creation myths in many religions involve the creation of Earth by a supernatural deity or deities. The Gaia hypothesis, developed in the mid-20th century, compared Earth's environments and life as a single self-regulating organism leading to broad stabilization of the conditions of habitability.Images of Earth taken from space, particularly during the Apollo program, have been credited with altering the way that people viewed the planet that they lived on, called the overview effect, emphasizing its beauty, uniqueness and apparent fragility. In particular, this caused a realization of the scope of effects from human activity on Earth's environment. Enabled by science, particularly Earth observation, humans have started to take action on environmental issues globally, acknowledging the impact of humans and the interconnectedness of Earth's environments. Scientific investigation has resulted in several culturally transformative shifts in people's view of the planet. Initial belief in a flat Earth was gradually displaced in Ancient Greece by the idea of a spherical Earth, which was attributed to both the philosophers Pythagoras and Parmenides. Earth was generally believed to be the center of the universe until the 16th century, when scientists first concluded that it was a moving object, one of the planets of the Solar System.It was only during the 19th century that geologists realized Earth's age was at least many millions of years. Lord Kelvin used thermodynamics to estimate the age of Earth to be between 20 million and 400 million years in 1864, sparking a vigorous debate on the subject; it was only when radioactivity and radioactive dating were discovered in the late 19th and early 20th centuries that a reliable mechanism for determining Earth's age was established, proving the planet to be billions of years old.     Earth – Profile – Solar System Exploration – NASA Earth Observatory – NASA Earth – Videos – International Space Station: Video (01:02) on YouTube – Earth (time-lapse) Video (00:27) on YouTube – Earth and auroras (time-lapse)\n",
      "\n",
      "---\n",
      "\n",
      "Input Text (Chemistry):\n",
      "The word chemistry comes from a modification during the Renaissance of the word alchemy, which referred to an earlier set of practices that encompassed elements of chemistry, metallurgy, philosophy, astrology, astronomy, mysticism, and medicine. Alchemy is often associated with the quest to turn lead or other base metals into gold, though alchemists were also interested in many of the questions of modern chemistry.The modern word alchemy in turn is derived from the Arabic word al-kīmīā (الكیمیاء). This may have Egyptian origins since al-kīmīā is derived from the Ancient Greek χημία, which is in turn derived from the word Kemet, which is the ancient name of Egypt in the Egyptian language. Alternately, al-kīmīā may derive from χημεία 'cast together'. The current model of atomic structure is the quantum mechanical model. Traditional chemistry starts with the study of elementary particles, atoms, molecules, substances, metals, crystals and other aggregates of matter. Matter can be studied in solid, liquid, gas and plasma states, in isolation or in combination. The interactions, reactions and transformations that are studied in chemistry are usually the result of interactions between atoms, leading to rearrangements of the chemical bonds which hold atoms together. Such behaviors are studied in a chemistry laboratory. The chemistry laboratory stereotypically uses various forms of laboratory glassware. However glassware is not central to chemistry, and a great deal of experimental (as well as applied/industrial) chemistry is done without it.  A chemical reaction is a transformation of some substances into one or more different substances. The basis of such a chemical transformation is the rearrangement of electrons in the chemical bonds between atoms. It can be symbolically depicted through a chemical equation, which usually involves atoms as subjects. The number of atoms on the left and the right in the equation for a chemical transformation is equal. (When the number of atoms on either side is unequal, the transformation is referred to as a nuclear reaction or radioactive decay.) The type of chemical reactions a substance may undergo and the energy changes that may accompany it are constrained by certain basic rules, known as chemical laws. Energy and entropy considerations are invariably important in almost all chemical studies. Chemical substances are classified in terms of their structure, phase, as well as their chemical compositions. They can be analyzed using the tools of chemical analysis, e.g. spectroscopy and chromatography. Scientists engaged in chemical research are known as chemists. Most chemists specialize in one or more sub-disciplines. Several concepts are essential for the study of chemistry; some of them are: The history of chemistry spans a period from very old times to the present. Since several millennia BC, civilizations were using technologies that would eventually form the basis of the various branches of chemistry. Examples include extracting metals from ores, making pottery and glazes, fermenting beer and wine, extracting chemicals from plants for medicine and perfume, rendering fat into soap, making glass, and making alloys like bronze. Chemistry was preceded by its protoscience, alchemy, which operated a non-scientific approach to understanding the constituents of matter and their interactions. Despite being unsuccessful in explaining the nature of matter and its transformations, alchemists set the stage for modern chemistry by performing experiments and recording the results. Robert Boyle, although skeptical of elements and convinced of alchemy, played a key part in elevating the \"sacred art\" as an independent, fundamental and philosophical discipline in his work The Sceptical Chymist (1661).While both alchemy and chemistry are concerned with matter and its transformations, the crucial difference was given by the scientific method that chemists employed in their work. Chemistry, as a body of knowledge distinct from alchemy, became an established science with the work of Antoine Lavoisier, who developed a law of conservation of mass that demanded careful measurement and quantitative observations of chemical phenomena. The history of chemistry afterwards is intertwined with the history of thermodynamics, especially through the work of Willard Gibbs.     Popular reading  Atkins, P.W. Galileo's Finger (Oxford University Press) ISBN 0-19-860941-8 Atkins, P.W. Atkins' Molecules (Cambridge University Press) ISBN 0-521-82397-8 Kean, Sam. The Disappearing Spoon – and Other True Tales from the Periodic Table (Black Swan) London, 2010 ISBN 978-0-552-77750-6 Levi, Primo The Periodic Table (Penguin Books) [1975] translated from the Italian by Raymond Rosenthal (1984) ISBN 978-0-14-139944-7  General Chemistry principles, patterns and applications.\n",
      "\n",
      "Resulting Slices:\n",
      "Slice 1: [ 1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  3  7  2  1  1  1  1  1\n",
      "  1  1  2  1  1  1  1  1  1  1  1  1  3  8  1  2  1  1  1  1  1  1  1  2\n",
      "  1  1  1  1  1  1  1  1  1  1 15  3 17  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  4  1  1  2  1  2  1  1  1  1  2  1  1\n",
      "  2  1  1]\n",
      "Slice 2: [1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 3 1 1 1 1\n",
      " 1 1 3 1 1 1 1 4 1 1 1 1 1 1 2 3 3 1 1 2 2 1 1 1 1 1 3 1 1 5 4 1 1 2 3 1 1\n",
      " 1 2 3 1 2 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1]\n",
      "Slice 3: [3 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 4 1 2 1 2 1 1 1 2 1 1 1 1 1\n",
      " 1 1 1 2 1 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 2 6 1 1 5 1 1 2 1 1 1 1 1 1 1 2 1\n",
      " 1 7 1 1 3 1 1 1 1 2 1 3 2 2 2 1 1 1 1 5 4 1 1 1 1]\n",
      "Input Text (Non-Chemistry):\n",
      "The general problem of simulating (or creating) intelligence has been broken into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research. AI research uses a wide variety of techniques to accomplish the goals above. AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). AI, like any powerful technology, has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to \"solve intelligence, and then use that to solve everything else\". However, as the use of AI has become widespread, several unintended consequences and risks have been identified.Anyone looking to use machine learning as part of real-world, in-production systems needs to factor ethics into their AI training processes and strive to avoid bias. This is especially true when using AI algorithms that are inherently unexplainable in deep learning. The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate both mathematical deduction and formal reasoning, which is known as the Church–Turing thesis. This, along with concurrent discoveries in cybernetics and information theory, led researchers to consider the possibility of building an \"electronic brain\".Alan Turing was thinking about machine intelligence at least as early as 1941, when he circulated a paper on machine intelligence which could be the earliest paper in the field of AI – though it is now lost. The first available paper generally recognized as \"AI\" was McCullouch and Pitts design for Turing-complete \"artificial neurons\" in 1943 – the first mathematical model of a neural network. The paper was influenced by Turing's earlier paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function.The term 'Machine Intelligence' was used by Alan Turing during his life which was later often referred to as 'Artificial Intelligence' after his death in 1954. In 1950 Turing published the best known of his papers 'Computing Machinery and Intelligence', the paper introduced his concept of what is now known as the Turing test to the general public. Then followed three radio broadcasts on AI by Turing, the lectures: 'Intelligent Machinery, A Heretical Theory’, ‘Can Digital Computers Think’? and the panel discussion ‘Can Automatic Calculating Machines be Said to Think’. By 1956 computer intelligence had been actively pursued for more than a decade in Britain; the earliest AI programmes were written there in 1951–1952.In 1951, using a Ferranti Mark 1 computer of the University of Manchester, checkers and chess programs were wrote where you could play against the computer. The field of American AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial Intelligence laboratories were set up at a number of British and US Universities in the latter 1950s and early 1960s.They had, however, underestimated the difficulty of the problem. Both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.Many researchers began to doubt that the current practices would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches. Robotics researchers, such as Rodney Brooks, rejected \"representation\" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\".Several academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field. For many specific tasks, other methods were abandoned. Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.   Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence. AI effect Artificial intelligence detection software – Software to detect AI-generated contentPages displaying short descriptions of redirect targets Artificial intelligence in healthcare – Overview of the use of artificial intelligence in healthcare Behavior selection algorithm – Algorithm that selects actions for intelligent agents Business process automation – Technology-enabled automation of complex business processes Case-based reasoning – Process of solving new problems based on the solutions of similar past problems     \"Artificial Intelligence\". Internet Encyclopedia of Philosophy. Thomason, Richmond. \"Logic and Artificial Intelligence\". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy. Artificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005). Theranostics and AI—The Next Advance in Cancer Precision Medicine.\n",
      "\n",
      "Resulting Slices:\n",
      "Slice 1: [ 1  1  1  2  3  1  1  2  1  1  1  1  1  1  1  1  3  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  2  2  1  1  1  1  1  1  1  1  2  2  1  1  1 28  3  1  1\n",
      "  1  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  2  2  1  2  2  1\n",
      "  1 17  3  1  1  1  1  2  2  1  1  1  1  2  1  2  2  2  6  2  1  1  1  1\n",
      "  1  1  1  2  1  1  1  3  1  1  1  1  1  2  1]\n",
      "Slice 2: [ 1  1  1  1  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1 13  1  1\n",
      "  1  1  1  1  1  3  1  1  1  1  3  1  3  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  5  1  1  3  1  1  2  3  1  2  1  1  2  2  2  1  3  2  1\n",
      "  1  1  1  1  1  1  1  4  1  2  1  1  1  1  1  1  1  1  1  2  2  1  1  2\n",
      "  1  2  1  1  1  2  1  1  1  2  1  1  1  1  1]\n",
      "Slice 3: [ 1  4  1  1  1  1  1  1  3  2  1  1  1  1  1  1  1  1  1  1  4  1  5  3\n",
      "  1  1  1  1  1  3  1  1  1  2  1  2  1  1  1  2  1  1  7  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  2  1  1  2  1  2  1  1  2  1  1  8  6  2  2  3  1\n",
      "  3  1  2  1  1  2  1  2  2  1  1  2  5  1  2  2  1  1  1  1  1  1 17  1\n",
      "  2  1  2  1  2  4  1  1  2  1  2  1  2  1  1]\n",
      "Slice 4: [ 1  1  4  1  1  1  2  1  2  1  3  1  1  1  1  2  4  5  1  1  1  3  1  1\n",
      "  1  1  1  1  1  1  1  1  1  2  2  1  1  5  1  1  1  1  2  4  3  2  1  3\n",
      "  1  1  1  1  2  1  1  3  1  1  1  3  1  1  1  1  2  1  2  1  2  1  1  2\n",
      "  3  1  1  1  7  1  1  1  1  1  1  1  1  1  2  4 14  1  1  1  1  1  1  3\n",
      "  2  1  2  1  3  1  2  2  1  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "\n",
    "def get_wikipedia_documents(topic, num_paragraphs=6):\n",
    "    wiki_wiki = wikipediaapi.Wikipedia('en', extract_format=wikipediaapi.ExtractFormat.WIKI, headers={'User-Agent': 'emilalizada0@gmail.com'})\n",
    "\n",
    "    page_py = wiki_wiki.page(topic)\n",
    "\n",
    "    if not page_py.exists():\n",
    "        return None\n",
    "\n",
    "    paragraphs = []\n",
    "    for section in page_py.sections:\n",
    "        paragraphs.extend(section.text.split('\\n')[:num_paragraphs])\n",
    "\n",
    "    return ' '.join(paragraphs)\n",
    "\n",
    "\n",
    "chemistry_topics = [\"Chemistry\", \"Acids\", \"Kinetics\", \"Atomic\", \"Stoichiometry\", \"Electrochemistry\"]\n",
    "non_chemistry_topics = [\"Artificial intelligence\", \"Computer Science\", \"Medical\", \"History\", \"Astronomy\", \"Earth\"]\n",
    "chemistry_documents = []\n",
    "non_chemistry_documents = []\n",
    "\n",
    "# Fetch documents for chemistry topics\n",
    "for topic in chemistry_topics:\n",
    "    document = get_wikipedia_documents(topic)\n",
    "    if document:\n",
    "        chemistry_documents.append(document)\n",
    "    else:\n",
    "        print(f\"Could not retrieve document for {topic}\")\n",
    "\n",
    "# Fetch documents for non-chemistry topics\n",
    "for topic in non_chemistry_topics:\n",
    "    document = get_wikipedia_documents(topic)\n",
    "    if document:\n",
    "        non_chemistry_documents.append(document)\n",
    "    else:\n",
    "        print(f\"Could not retrieve document for {topic}\")\n",
    "\n",
    "# Displaying\n",
    "print(\"Chemistry Documents:\")\n",
    "for i, document in enumerate(chemistry_documents, start=1):\n",
    "    print(f\"Document {i} ({chemistry_topics[i-1]}):\")\n",
    "    print(document)\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "print(\"\\nNon-Chemistry Documents:\")\n",
    "for i, document in enumerate(non_chemistry_documents, start=1):\n",
    "    print(f\"Document {i} ({non_chemistry_topics[i-1]}):\")\n",
    "    print(document)\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "# Select one document from each category for NLP pipeline\n",
    "sample_input1 = chemistry_documents[0] if chemistry_documents else None\n",
    "sample_input2 = non_chemistry_documents[0] if non_chemistry_documents else None\n",
    "\n",
    "if sample_input1:\n",
    "    result_slices1 = nlp_pipeline(sample_input1)\n",
    "    print(\"Input Text (Chemistry):\")\n",
    "    print(sample_input1)\n",
    "    print(\"\\nResulting Slices:\")\n",
    "    for i, slice in enumerate(result_slices1, start=1):\n",
    "        print(f\"Slice {i}: {slice}\")\n",
    "\n",
    "if sample_input2:\n",
    "    result_slices2 = nlp_pipeline(sample_input2)\n",
    "    print(\"Input Text (Non-Chemistry):\")\n",
    "    print(sample_input2)\n",
    "    print(\"\\nResulting Slices:\")\n",
    "    for i, slice in enumerate(result_slices2, start=1):\n",
    "        print(f\"Slice {i}: {slice}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words for Slices Below Standard Size:\n",
      "Slice 1: [ 1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  1  3  7  2  1  1  1  1  1\n",
      "  1  1  2  1  1  1  1  1  1  1  1  1  3  8  1  2  1  1  1  1  1  1  1  2\n",
      "  1  1  1  1  1  1  1  1  1  1 15  3 17  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  4  1  1  2  1  2  1  1  1  1  2  1  1\n",
      "  2  1  1]\n",
      "Slice 2: [1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 3 1 1 1 1\n",
      " 1 1 3 1 1 1 1 4 1 1 1 1 1 1 2 3 3 1 1 2 2 1 1 1 1 1 3 1 1 5 4 1 1 2 3 1 1\n",
      " 1 2 3 1 2 1 1 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1]\n",
      "Slice 3: [3 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 4 1 2 1 2 1 1 1 2 1 1 1 1 1\n",
      " 1 1 1 2 1 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 2 6 1 1 5 1 1 2 1 1 1 1 1 1 1 2 1\n",
      " 1 7 1 1 3 1 1 1 1 2 1 3 2 2 2 1 1 1 1 5 4 1 1 1 1]\n",
      "\n",
      "Bag of Words for Slices Above Standard Size:\n",
      "Slice 1: [ 100  100  100  100  100  100  100  100  100  100  100  100  200  100\n",
      "  100  100  300  700  200  100  100  100  100  100  100  100  200  100\n",
      "    1   99  100  100  100  100  100  100  100  300  800  100  200  100\n",
      "  100  100  100  100  100  100  200  100  100  100  100  100  100  100\n",
      "  100  100  100 1500  300 1700  100  100  100  100  100  100  100  100\n",
      "  100  100  100  100  100  100  100  100  100  100  100  100  100  100\n",
      "  400  100  100  200  100  200  100  100  100  100  200  100  100  200\n",
      "  100]\n",
      "Slice 2: [100 100 100 200 100 100 100 200 100 100 100 100 100 100 100 100 200 100\n",
      " 100 100 200 100 100 100 100 100 100 100 200 100 100 100 100 300 100 100\n",
      " 100 100 100 100 300 100 100 100 100 400 100 100 100 100 100 100 200 300\n",
      " 300 100 100 200 200 100 100 100 100 100 300 100 100 500 400 100 100 200\n",
      " 300 100 100 100 200 300 100 200 100 100 100 100 100 200 100 100 100 200\n",
      " 100 100 100 100 100 100 100 100 100]\n",
      "Slice 3: [100 300 100 100 100 100 100 100 100 100 100 100 100 100 200 100 100 100\n",
      " 100 100 100 100 100 100 400 100 200 100 200 100 100 100 200 100 100 100\n",
      " 100 100 100 100 100 200 100 200 200 100 200 100 100 100 100 100 100 100\n",
      " 100 100 100 100 200 600 100 100 500 100 100 200 100 100 100 199 100 100\n",
      " 100 200 100 100 700 100 100 300 100 100 100 100 200 100 300 200 200 200\n",
      " 100 100 100 100 500 400 100 100 100]\n",
      "Slice 4: [100]\n",
      "\n",
      "Cosine Distances for Slices Below Standard Size:\n",
      "Distance between Slice 1 and Slice 2: 0.5475807801573277\n",
      "Distance between Slice 2 and Slice 3: 0.324331737651404\n",
      "\n",
      "Cosine Distances for Slices Above Standard Size:\n",
      "Distance between Slice 1 and Slice 2: 0.5472651081063096\n",
      "Distance between Slice 2 and Slice 3: 0.32390058640806885\n",
      "Distance between Slice 3 and Slice 4: 0.0\n",
      "\n",
      "Testing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_cosine_distance(slice1, slice2):\n",
    "    # Ensure the input slices are NumPy arrays\n",
    "    vector1 = np.array(slice1)\n",
    "    vector2 = np.array(slice2)\n",
    "\n",
    "    common_dimensions = min(len(vector1), len(vector2))\n",
    "    vector1 = vector1[:common_dimensions]\n",
    "    vector2 = vector2[:common_dimensions]\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))\n",
    "    cosine_distance = 1 - similarity_matrix[0, 0]\n",
    "\n",
    "    return cosine_distance\n",
    "\n",
    "# Testing with Different Inputs\n",
    "cosine_distance_threshold = 0.2\n",
    "\n",
    "#Chemistry\n",
    "input_below_standard_size = sample_input1\n",
    "slices_below_standard_size = nlp_pipeline(input_below_standard_size)\n",
    "\n",
    "#Chemistry, Above Standard Size\n",
    "input_above_standard_size = sample_input1 * 100\n",
    "slices_above_standard_size = nlp_pipeline(input_above_standard_size)\n",
    "\n",
    "# Verify bag-of-words representations for slices\n",
    "print(\"Bag of Words for Slices Below Standard Size:\")\n",
    "for i, slice in enumerate(slices_below_standard_size, start=1):\n",
    "    print(f\"Slice {i}: {slice}\")\n",
    "\n",
    "print(\"\\nBag of Words for Slices Above Standard Size:\")\n",
    "for i, slice in enumerate(slices_above_standard_size, start=1):\n",
    "    print(f\"Slice {i}: {slice}\")\n",
    "\n",
    "# Verify Cosine Distances for Adjacent Slices\n",
    "print(\"\\nCosine Distances for Slices Below Standard Size:\")\n",
    "for i in range(len(slices_below_standard_size) - 1):\n",
    "    distance = calculate_cosine_distance(slices_below_standard_size[i], slices_below_standard_size[i+1])\n",
    "    print(f\"Distance between Slice {i+1} and Slice {i+2}: {distance}\")\n",
    "\n",
    "print(\"\\nCosine Distances for Slices Above Standard Size:\")\n",
    "for i in range(len(slices_above_standard_size) - 1):\n",
    "    distance = calculate_cosine_distance(slices_above_standard_size[i], slices_above_standard_size[i+1])\n",
    "    print(f\"Distance between Slice {i+1} and Slice {i+2}: {distance}\")\n",
    "\n",
    "print(\"\\nTesting completed successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
